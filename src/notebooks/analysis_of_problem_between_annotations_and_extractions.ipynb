{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGuUhN-pSrDO",
        "outputId": "c4eb9140-fc02-4397-e034-674db53da9d6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n",
        "%cd \"/content/drive/MyDrive/Colab Notebooks/skema/data\""
      ],
      "metadata": {
        "id": "svHXmlE2TjGn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6674a4a-d5ac-4786-bcf7-65920c53f1cc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data\n",
            "/content/drive/MyDrive/Colab Notebooks/skema/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GDEh1xFgFuaO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "import io"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/Colab Notebooks/skema/data/\"\n",
        "extractions_path = \"cosmos-and-extractions-jsons-for-3-papers\"\n",
        "annotated_files = [\"data-sars-double.json\", \"data_modeling_covid_italy.json\", \"data-response-to-covid-19-was-italy-unprepared.json\"]\n",
        "extraction_files =[\"extractions_sarsdouble.json\", \"extractions_modeling_covid_italy--COSMOS-data.json\",\"extractions_response-to-covid-19-was-italy-unprepared--COSMOS-data.json\"]\n",
        "paper_names = [\"sarsdouble.pdf\", \"modeling_covid_italy.pdf\", \"response-to-covid-19-was-italy-unprepared.pdf\"]\n",
        "ann_extr_file_pairs = {}\n",
        "for name,ann, extr in zip(paper_names,annotated_files, extraction_files):\n",
        "  ann_extr_file_pairs[name] = [ann, extr]"
      ],
      "metadata": {
        "id": "q9kCzmA7SrBi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ann_extr_file_pairs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3MqY_7UUWYh",
        "outputId": "72f1230d-8887-4385-9f48-308bf7c72580"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sarsdouble.pdf': ['data-sars-double.json', 'extractions_sarsdouble.json'],\n",
              " 'modeling_covid_italy.pdf': ['data_modeling_covid_italy.json',\n",
              "  'extractions_modeling_covid_italy--COSMOS-data.json'],\n",
              " 'response-to-covid-19-was-italy-unprepared.pdf': ['data-response-to-covid-19-was-italy-unprepared.json',\n",
              "  'extractions_response-to-covid-19-was-italy-unprepared--COSMOS-data.json']}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save Combined annotations and extractions details in csv and json"
      ],
      "metadata": {
        "id": "Lj9SQNyYbiBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_extr_ann_file(path, filename, map):\n",
        "  pd.DataFrame.from_records(map).to_csv(os.path.join(path, filename+\".xlsx\"))\n",
        "\n",
        "  with io.open(os.path.join(path, filename+\".json\"), 'w', encoding='utf-8') as f:\n",
        "    f.write(json.dumps(map, ensure_ascii=False))"
      ],
      "metadata": {
        "id": "hParzgP0biS7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate combined annotations and extractions file"
      ],
      "metadata": {
        "id": "4bnqSLBha2Mj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_ann_extr(path, extractions_path, extr, ann,filename ):\n",
        "  with open(os.path.join(path,extractions_path, extr ), \"r\", encoding='UTF-8') as f:\n",
        "    contents = f.readlines()\n",
        "    extractions = json.loads(contents[0])\n",
        "  with open(os.path.join(path, ann ), \"r\", encoding='UTF-8') as f:\n",
        "    contents = f.read()\n",
        "    annotations = json.loads(contents)\n",
        "\n",
        "  print(len(annotations), len(extractions[\"mentions\"]), len(extractions[\"documents\"]))\n",
        "\n",
        "  doc_event_map = []\n",
        "  event_doc_map = {}\n",
        "  for mention in extractions['mentions']:\n",
        "    if mention['id'].startswith(\"E:\"):\n",
        "      for att in mention[\"attachments\"]:\n",
        "          if \"pageNum\" in att.keys():\n",
        "            doc_event_map.append({\"doc_id\":mention['document'], \"pg_num\":att[\"pageNum\"][0],\"blk_id\":att[\"blockIdx\"][0],\"sentence_id\":mention['sentence'], \n",
        "                                  \"doc_sentence_count\":len(extractions[\"documents\"][mention['document']][\"sentences\"]), \n",
        "                                  \"event_id\":mention[\"id\"], \"event\":mention[\"text\"] }) #corrected_sent_number -> 1,2,5,4,6,7, => 1,2,3,4,5,6\n",
        "            event_doc_map[mention[\"id\"]] = {\"doc_id\":mention['document'],\"pg_num\":att[\"pageNum\"][0],\"blk_id\":att[\"blockIdx\"][0],\"sentence_id\":mention['sentence'], \n",
        "                                  \"doc_sentence_count\":len(extractions[\"documents\"][mention['document']][\"sentences\"]), \n",
        "                                  \"event_id\":mention[\"id\"], \"event\":mention[\"text\"] }\n",
        "  event_ann_map = {}\n",
        "  for ann in annotations:\n",
        "    event_ann_map[ann['eventId']] = {\"annotated_page_num\":ann[\"page_num\"],\"para_num\":ann[\"para_num\"], \"event\":ann[\"event\"], 'locationContext': ann['locationContext'],\n",
        "    'temporalContext': ann['temporalContext'],'explanation': ann['explanation']}\n",
        "  \n",
        "  event_extr_ann_map = []\n",
        "  for event, values in event_doc_map.items():\n",
        "    this_event = event_ann_map[event]\n",
        "    event_extr_ann_map.append({\"doc_id\":values['doc_id'],\"annotated_page_num\":this_event[\"page_num\"],\"para_num\":this_event[\"para_num\"], \"event_id\":values[\"event_id\"],\n",
        "                               \"event\":this_event[\"event\"], 'locationContext': \",\".join(this_event['locationContext']),\n",
        "    'temporalContext': \",\".join(this_event['temporalContext']),'explanation': this_event['explanation'], 'pg_num':values['pg_num'], 'blk_id':values['blk_id'], \n",
        "    'sentence_id':values['sentence_id'], 'doc_sentence_count':values['doc_sentence_count']})\n",
        "  \n",
        "  save_extr_ann_file(path, filename, event_extr_ann_map)\n",
        "  "
      ],
      "metadata": {
        "id": "RYZh6iRRa1jT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Covid papers, Text Reading Pipeline Extractions & Annotations"
      ],
      "metadata": {
        "id": "RpUNcziLUsc5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for key in paper_names:\n",
        "#   name = key\n",
        "#   ann, extr = ann_extr_file_pairs[name]\n",
        "#   combine_ann_extr(path, extractions_path, extr, ann, name[:-4])"
      ],
      "metadata": {
        "id": "J3_-x8jzjWX-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_ann_extr_all(path, extractions_path, extr, ann,filename ):\n",
        "  with open(os.path.join(path,extractions_path, extr ), \"r\", encoding='UTF-8') as f:\n",
        "    contents = f.readlines()\n",
        "    extractions = json.loads(contents[0])\n",
        "  with open(os.path.join(path, ann ), \"r\", encoding='UTF-8') as f:\n",
        "    contents = f.read()\n",
        "    annotations = json.loads(contents)\n",
        "\n",
        "  print(len(annotations), len(extractions[\"mentions\"]), len(extractions[\"documents\"]))\n",
        "\n",
        "  doc_event_map = []\n",
        "  event_doc_map = {}\n",
        "  for mention in extractions['mentions']:\n",
        "    # if mention['id'].startswith(\"E:\"):\n",
        "    for att in mention[\"attachments\"]:\n",
        "      if \"pageNum\" in att.keys():\n",
        "        doc_event_map.append({\"doc_id\":mention['document'], \"pg_num\":att[\"pageNum\"][0],\"blk_id\":att[\"blockIdx\"][0],\"sentence_id\":mention['sentence'], \n",
        "                              \"doc_sentence_count\":len(extractions[\"documents\"][mention['document']][\"sentences\"]), \n",
        "                              \"event_id\":mention[\"id\"], \"event\":mention[\"text\"] }) #corrected_sent_number -> 1,2,5,4,6,7, => 1,2,3,4,5,6\n",
        "        event_doc_map[mention[\"id\"]] = {\"doc_id\":mention['document'],\"pg_num\":att[\"pageNum\"][0],\"blk_id\":att[\"blockIdx\"][0],\"sentence_id\":mention['sentence'], \n",
        "                              \"doc_sentence_count\":len(extractions[\"documents\"][mention['document']][\"sentences\"]), \n",
        "                              \"event_id\":mention[\"id\"], \"event\":mention[\"text\"] }\n",
        "  event_ann_map = {}\n",
        "  for ann in annotations:\n",
        "    event_ann_map[ann['eventId']] = {\"annotated_page_num\":ann[\"page_num\"],\"para_num\":ann[\"para_num\"], \"event\":ann[\"event\"], 'locationContext': ann['locationContext'],\n",
        "    'temporalContext': ann['temporalContext'],'explanation': ann['explanation']}\n",
        "  \n",
        "  empty_map = {\"annotated_page_num\":\"\",\"para_num\":\"\", \"event\":\"\", 'locationContext': \"\",\n",
        "    'temporalContext': \"\",'explanation': \"\"}\n",
        "  event_extr_ann_map = []\n",
        "  for event, values in event_doc_map.items():\n",
        "    this_event = event_ann_map[event] if event in event_ann_map.keys() else empty_map\n",
        "    event_extr_ann_map.append({\"doc_id\":values['doc_id'],\"annotated_page_num\":this_event[\"annotated_page_num\"],\"para_num\":this_event[\"para_num\"], \"event_id\":values[\"event_id\"],\n",
        "                               \"event\":this_event[\"event\"], 'locationContext': \",\".join(this_event['locationContext']),\n",
        "    'temporalContext': \",\".join(this_event['temporalContext']),'explanation': this_event['explanation'], 'pg_num':values['pg_num'], 'blk_id':values['blk_id'], \n",
        "    'sentence_id':values['sentence_id'], 'doc_sentence_count':values['doc_sentence_count']})\n",
        "\n",
        "  df = pd.DataFrame.from_records(event_extr_ann_map)\n",
        "  df = df[df.columns]\n",
        "  print(df.columns.to_list())\n",
        "  print(\"\\n\")\n",
        "  df['locationContext'] = df['locationContext'].replace({\"^'|'$\": \"\"}, regex=True)\n",
        "  df['temporalContext'] = df['temporalContext'].replace({\"^'|'$\": \"\"}, regex=True)\n",
        "  df.sort_values(by=['doc_id', 'pg_num','blk_id', 'sentence_id', 'doc_sentence_count'], inplace=True,\n",
        "               ascending = [True, True, True, True, True])\n",
        "  d = df[['doc_id', 'pg_num','blk_id','sentence_id', 'doc_sentence_count']]\n",
        "  d['linear_order'] = [i for i in range( 1,len(d)+1, 1)]\n",
        "  d.to_csv(os.path.join(path, filename+\"_linear_order\"+\".xlsx\"))\n",
        "  save_extr_ann_file(path, filename, event_extr_ann_map)\n",
        "  "
      ],
      "metadata": {
        "id": "g_Ngw0MflxIH"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key in paper_names:\n",
        "  name = key\n",
        "  ann, extr = ann_extr_file_pairs[name]\n",
        "  combine_ann_extr_all(path, extractions_path, extr, ann, name[:-4]+\"_all\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgrEclr9S8Yb",
        "outputId": "c0d0b5f5-8936-430a-f16b-8494f7298deb"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "174 6212 91\n",
            "['doc_id', 'annotated_page_num', 'para_num', 'event_id', 'event', 'locationContext', 'temporalContext', 'explanation', 'pg_num', 'blk_id', 'sentence_id', 'doc_sentence_count']\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-79a30e2aa8d0>:47: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  d['linear_order'] = [i for i in range( 1,len(d)+1, 1)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "302 10045 76\n",
            "['doc_id', 'annotated_page_num', 'para_num', 'event_id', 'event', 'locationContext', 'temporalContext', 'explanation', 'pg_num', 'blk_id', 'sentence_id', 'doc_sentence_count']\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-79a30e2aa8d0>:47: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  d['linear_order'] = [i for i in range( 1,len(d)+1, 1)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "42 4429 47\n",
            "['doc_id', 'annotated_page_num', 'para_num', 'event_id', 'event', 'locationContext', 'temporalContext', 'explanation', 'pg_num', 'blk_id', 'sentence_id', 'doc_sentence_count']\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-79a30e2aa8d0>:47: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  d['linear_order'] = [i for i in range( 1,len(d)+1, 1)]\n"
          ]
        }
      ]
    }
  ]
}