{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Approach for searching the contexts\n",
        "- For each pdf in pdf_names:\n",
        "  - First construct contexts dictionary with contexts as keys and doc_id, block_id, sentence_number, **all extractions' linear_order_number** from {this_pdf_name}_events.xlsx\n",
        "  - Get doc_sentence_map with doc_id, sentence_text, sentence_id, doc_sent_linear_order from {this_pdf_name}_all.csv\n",
        "  - For each context key in contexts dictionary :\n",
        "    - this_doc_id, sentence_number from contexts dictionary\n",
        "    - from doc_sentence_map, get all sentences up until **doc_sent_map's doc_sent_linear_order**\n",
        "    - get 3 top closest doc_id, sentence_number pairs where the context was found up until this_doc_id,this_sentence_number\n",
        "    - calculate distance between **doc_sent_map's doc_sent_linear_order** between context dictionary value and doc_sentence_map for each of the nearest context matches\n",
        "- Plot the histogram\n",
        "\n",
        "## Approach for ordering the documents, extractions\n",
        "- For each pdf in pdf_names:\n",
        "  - First sort doc_id, block_id, sentence_number and add linear_order_number from {this_pdf_name}_all_linear_order.xlsx\n",
        "  - Construct doc_sentence_map with doc_id, sentence_text, sentence_id, and add doc_sent_linear_order from {this_pdf_name}_all.csv\n",
        "  - For each of the sorted doc_id, block_id, sentence_number and linear_order_number, add doc_sent_linear_order.\n",
        "  - Now only save events from this list doc_id, block_id, sentence_number and linear_order_number and doc_sent_linear_order along with contexts and sentence text {this_pdf_name}_events.xlsx\n",
        "  - This is because mentions[\"documents\"] and mentions[\"extractions\"] do not match. mentions[\"extractions\"] have page numbers and block numbers. But again COSMOS JSON does not have block numbers. So we need to sort in this order doc_id, block_id, sentence_number and add linear_order_number for extractions, then a linear order number for documents after sorting doc_ids and sentence_numbers. And combine the two for searching the contexts within documents by document linear order."
      ],
      "metadata": {
        "id": "YxqsH3hu3feM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install altair\n",
        "!pip install altair vega_datasets\n",
        "!pip install vega\n",
        "!pip install altair_viewer\n",
        "!pip install textwrap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "al3Jp0mixieK",
        "outputId": "2869f2f7-026b-4aab-e579-7ac7e88e0fa6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: altair in /usr/local/lib/python3.10/dist-packages (4.2.2)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair) (3.1.2)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair) (4.3.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from altair) (1.22.4)\n",
            "Requirement already satisfied: pandas>=0.18 in /usr/local/lib/python3.10/dist-packages (from altair) (1.5.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair) (0.12.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair) (23.1.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair) (0.19.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.18->altair) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.18->altair) (2022.7.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair) (2.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=0.18->altair) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: altair in /usr/local/lib/python3.10/dist-packages (4.2.2)\n",
            "Requirement already satisfied: vega_datasets in /usr/local/lib/python3.10/dist-packages (0.9.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair) (3.1.2)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair) (4.3.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from altair) (1.22.4)\n",
            "Requirement already satisfied: pandas>=0.18 in /usr/local/lib/python3.10/dist-packages (from altair) (1.5.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair) (0.12.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair) (23.1.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair) (0.19.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.18->altair) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.18->altair) (2022.7.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair) (2.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=0.18->altair) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting vega\n",
            "  Downloading vega-4.0.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ipytablewidgets<0.4.0,>=0.3.0 (from vega)\n",
            "  Downloading ipytablewidgets-0.3.1-py2.py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.2/190.2 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jupyter<2.0.0,>=1.0.0 (from vega)\n",
            "  Downloading jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\n",
            "Requirement already satisfied: pandas<2.0.0,>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from vega) (1.5.3)\n",
            "Requirement already satisfied: ipywidgets<9,>=7.5.0 in /usr/local/lib/python3.10/dist-packages (from ipytablewidgets<0.4.0,>=0.3.0->vega) (7.7.1)\n",
            "Requirement already satisfied: traitlets>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from ipytablewidgets<0.4.0,>=0.3.0->vega) (5.7.1)\n",
            "Collecting traittypes>=0.0.6 (from ipytablewidgets<0.4.0,>=0.3.0->vega)\n",
            "  Downloading traittypes-0.2.1-py2.py3-none-any.whl (8.6 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.10.4 in /usr/local/lib/python3.10/dist-packages (from ipytablewidgets<0.4.0,>=0.3.0->vega) (1.22.4)\n",
            "Collecting lz4 (from ipytablewidgets<0.4.0,>=0.3.0->vega)\n",
            "  Downloading lz4-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: notebook in /usr/local/lib/python3.10/dist-packages (from jupyter<2.0.0,>=1.0.0->vega) (6.4.8)\n",
            "Collecting qtconsole (from jupyter<2.0.0,>=1.0.0->vega)\n",
            "  Downloading qtconsole-5.4.3-py3-none-any.whl (121 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.9/121.9 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jupyter-console in /usr/local/lib/python3.10/dist-packages (from jupyter<2.0.0,>=1.0.0->vega) (6.1.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (from jupyter<2.0.0,>=1.0.0->vega) (6.5.4)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (from jupyter<2.0.0,>=1.0.0->vega) (5.5.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.0.0,>=1.5.0->vega) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.0.0,>=1.5.0->vega) (2022.7.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<9,>=7.5.0->ipytablewidgets<0.4.0,>=0.3.0->vega) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<9,>=7.5.0->ipytablewidgets<0.4.0,>=0.3.0->vega) (3.6.4)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<9,>=7.5.0->ipytablewidgets<0.4.0,>=0.3.0->vega) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<9,>=7.5.0->ipytablewidgets<0.4.0,>=0.3.0->vega) (3.0.7)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter<2.0.0,>=1.0.0->vega) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter<2.0.0,>=1.0.0->vega) (6.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas<2.0.0,>=1.5.0->vega) (1.16.0)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-console->jupyter<2.0.0,>=1.0.0->vega) (3.0.38)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from jupyter-console->jupyter<2.0.0,>=1.0.0->vega) (2.14.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter<2.0.0,>=1.0.0->vega) (4.9.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter<2.0.0,>=1.0.0->vega) (4.11.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter<2.0.0,>=1.0.0->vega) (6.0.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter<2.0.0,>=1.0.0->vega) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter<2.0.0,>=1.0.0->vega) (0.4)\n",
            "Requirement already satisfied: jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter<2.0.0,>=1.0.0->vega) (3.1.2)\n",
            "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter<2.0.0,>=1.0.0->vega) (5.3.0)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter<2.0.0,>=1.0.0->vega) (0.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter<2.0.0,>=1.0.0->vega) (2.1.2)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter<2.0.0,>=1.0.0->vega) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter<2.0.0,>=1.0.0->vega) (0.7.4)\n",
            "Requirement already satisfied: nbformat>=5.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter<2.0.0,>=1.0.0->vega) (5.8.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter<2.0.0,>=1.0.0->vega) (23.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter<2.0.0,>=1.0.0->vega) (1.5.0)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter<2.0.0,>=1.0.0->vega) (1.2.1)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter<2.0.0,>=1.0.0->vega) (23.2.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter<2.0.0,>=1.0.0->vega) (21.3.0)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter<2.0.0,>=1.0.0->vega) (1.5.6)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter<2.0.0,>=1.0.0->vega) (1.8.0)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter<2.0.0,>=1.0.0->vega) (0.17.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter<2.0.0,>=1.0.0->vega) (0.16.0)\n",
            "Collecting qtpy>=2.0.1 (from qtconsole->jupyter<2.0.0,>=1.0.0->vega)\n",
            "  Downloading QtPy-2.3.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.9/84.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets<9,>=7.5.0->ipytablewidgets<0.4.0,>=0.3.0->vega) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython>=4.0.0->ipywidgets<9,>=7.5.0->ipytablewidgets<0.4.0,>=0.3.0->vega)\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets<9,>=7.5.0->ipytablewidgets<0.4.0,>=0.3.0->vega) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets<9,>=7.5.0->ipytablewidgets<0.4.0,>=0.3.0->vega) (0.7.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets<9,>=7.5.0->ipytablewidgets<0.4.0,>=0.3.0->vega) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets<9,>=7.5.0->ipytablewidgets<0.4.0,>=0.3.0->vega) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets<9,>=7.5.0->ipytablewidgets<0.4.0,>=0.3.0->vega) (4.8.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.7->nbconvert->jupyter<2.0.0,>=1.0.0->vega) (3.3.0)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert->jupyter<2.0.0,>=1.0.0->vega) (2.16.3)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert->jupyter<2.0.0,>=1.0.0->vega) (4.3.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter<2.0.0,>=1.0.0->vega) (0.2.6)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.10/dist-packages (from terminado>=0.8.3->notebook->jupyter<2.0.0,>=1.0.0->vega) (0.7.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook->jupyter<2.0.0,>=1.0.0->vega) (21.2.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert->jupyter<2.0.0,>=1.0.0->vega) (2.4.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert->jupyter<2.0.0,>=1.0.0->vega) (0.5.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets<9,>=7.5.0->ipytablewidgets<0.4.0,>=0.3.0->vega) (0.8.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter<2.0.0,>=1.0.0->vega) (23.1.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter<2.0.0,>=1.0.0->vega) (0.19.3)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter<2.0.0,>=1.0.0->vega) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter<2.0.0,>=1.0.0->vega) (2.21)\n",
            "Installing collected packages: traittypes, qtpy, lz4, jedi, qtconsole, jupyter, ipytablewidgets, vega\n",
            "Successfully installed ipytablewidgets-0.3.1 jedi-0.18.2 jupyter-1.0.0 lz4-4.3.2 qtconsole-5.4.3 qtpy-2.3.1 traittypes-0.2.1 vega-4.0.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting altair_viewer\n",
            "  Downloading altair_viewer-0.4.0-py3-none-any.whl (844 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m844.5/844.5 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: altair in /usr/local/lib/python3.10/dist-packages (from altair_viewer) (4.2.2)\n",
            "Collecting altair-data-server>=0.4.0 (from altair_viewer)\n",
            "  Downloading altair_data_server-0.4.1-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: portpicker in /usr/local/lib/python3.10/dist-packages (from altair-data-server>=0.4.0->altair_viewer) (1.3.9)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.10/dist-packages (from altair-data-server>=0.4.0->altair_viewer) (6.3.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair->altair_viewer) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair->altair_viewer) (3.1.2)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair->altair_viewer) (4.3.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from altair->altair_viewer) (1.22.4)\n",
            "Requirement already satisfied: pandas>=0.18 in /usr/local/lib/python3.10/dist-packages (from altair->altair_viewer) (1.5.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair->altair_viewer) (0.12.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair->altair_viewer) (23.1.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair->altair_viewer) (0.19.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.18->altair->altair_viewer) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.18->altair->altair_viewer) (2022.7.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair->altair_viewer) (2.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=0.18->altair->altair_viewer) (1.16.0)\n",
            "Installing collected packages: altair-data-server, altair_viewer\n",
            "Successfully installed altair-data-server-0.4.1 altair_viewer-0.4.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement textwrap (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for textwrap\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "333Evgoyx3C6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "import io"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNDGN3k3zUnX",
        "outputId": "071946fa-c74b-4536-84a5-7f32ab44e464"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n",
        "%cd \"/content/drive/MyDrive/Colab Notebooks/skema/data\"\n",
        "!ls"
      ],
      "metadata": {
        "id": "K7GbECKOz2wZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/Colab Notebooks/skema/data/\"\n",
        "os.path.exists(path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQFSQRYTzU0h",
        "outputId": "87777db6-c873-4d33-e463-1354f6b69a7e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filenames = [\"sarsdouble.xlsx\", \"modeling_covid_italy.xlsx\", \"response-to-covid-19-was-italy-unprepared.xlsx\"]\n",
        "diff_distance_map = []\n",
        "for filename in filenames:\n",
        "  print(\"\\n**********************************************************************************\\n\")\n",
        "  print(\"The annotated extractions file name %s \" %(filename))\n",
        "  df = pd.read_csv(os.path.join(path, filename))\n",
        "  df['locationContext'] = df['locationContext'].replace({\"^'|'$\": \"\"}, regex=True)\n",
        "  df['temporalContext'] = df['temporalContext'].replace({\"^'|'$\": \"\"}, regex=True)\n",
        "\n",
        "  print(\"Number of page numbers different from extractions with that of manually annotated are : %d out of %d\" %(len(df[df['pg_num'] != df[\"page_num\"]]), df.shape[0]) )\n",
        "  gp = df.groupby(['locationContext']).count()\n",
        "  print(gp.reset_index()[['locationContext', 'event_id']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9v7bCqh0Bh-",
        "outputId": "ecc5ceae-24c7-4caa-fe95-320d7a06f746"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "**********************************************************************************\n",
            "\n",
            "The annotated extractions file name sarsdouble.xlsx \n",
            "Number of page numbers different from extractions with that of manually annotated are : 11 out of 172\n",
            "                                      locationContext  event_id\n",
            "0                                             Beijing        17\n",
            "1                                     China,Hong Kong        15\n",
            "2                    China,Hong Kong,Beijing,Shanghai         1\n",
            "3     China,Hong Kong,Mainland China,Beijing,Shanghai         1\n",
            "4             Europe,Inner Mongolia,Hong Kong,Beijing         1\n",
            "5   Guangdong Province,Hong Kong,China,Mainland Ch...         2\n",
            "6   Guangdong Province,Hong Kong,Mainland China,Be...         2\n",
            "7                           Guangdong,China,Hong Kong         1\n",
            "8                                           Hong Kong        43\n",
            "9                              Hong Kong,Amoy Gardens        21\n",
            "10                                Hong Kong,Guangdong        23\n",
            "11                       Hong Kong,Guangdong Province         3\n",
            "12                                     Inner Mongolia        23\n",
            "13                   Inner Mongolia,Hong Kong,Beijing        14\n",
            "14             Inner Mongolia,Hong Kong,Beijing,Paris         1\n",
            "\n",
            "**********************************************************************************\n",
            "\n",
            "The annotated extractions file name modeling_covid_italy.xlsx \n",
            "Number of page numbers different from extractions with that of manually annotated are : 87 out of 302\n",
            "                            locationContext  event_id\n",
            "0  Italian population,Italian society,Italy         1\n",
            "1                                     Italy       249\n",
            "2                               Italy,China         1\n",
            "3              Italy,Lodi Province,Lombardy         1\n",
            "4         Italy,north to the south of Italy         1\n",
            "\n",
            "**********************************************************************************\n",
            "\n",
            "The annotated extractions file name response-to-covid-19-was-italy-unprepared.xlsx \n",
            "Number of page numbers different from extractions with that of manually annotated are : 5 out of 42\n",
            "                                      locationContext  event_id\n",
            "0                                             Italian         1\n",
            "1                                               Italy        26\n",
            "2       Italy,European countries and the USA,Lombardy         2\n",
            "3                               Italy,Italian regions         1\n",
            "4                                      Italy,Lombardy         1\n",
            "5                                 Italy,Lombardy,Rome         1\n",
            "6   Italy,Lombardy,Rome,western European countries...         1\n",
            "7                                Italy,northern Italy         2\n",
            "8                         Italy,northern regions,Rome         1\n",
            "9                                            Lombardy         1\n",
            "10                                     Lombardy,Italy         1\n",
            "11      Lombardy,Veneto,Emilia-Romagna,Piedmont,Italy         1\n",
            "12                                         Rome,Italy         1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Distances between annotated page numbers cosmos page numbers: \", len(df[df['pg_num'] != df[\"page_num\"]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6GLlKn3qZtw",
        "outputId": "1f991878-078a-404c-d97f-c387b0ab24e8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distances between annotated page numbers cosmos page numbers:  5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/Colab Notebooks/skema/data/\"\n",
        "extractions_path = \"cosmos-and-extractions-jsons-for-3-papers\"\n",
        "annotated_files = [\"data-sars-double.json\", \"data_modeling_covid_italy.json\", \"data-response-to-covid-19-was-italy-unprepared.json\"]\n",
        "extraction_files =[\"extractions_sarsdouble.json\", \"extractions_modeling_covid_italy--COSMOS-data.json\",\"extractions_response-to-covid-19-was-italy-unprepared--COSMOS-data.json\"]\n",
        "paper_names = [\"sarsdouble.pdf\", \"modeling_covid_italy.pdf\", \"response-to-covid-19-was-italy-unprepared.pdf\"]\n",
        "ann_extr_file_pairs = {}\n",
        "for name,ann, extr in zip(paper_names,annotated_files, extraction_files):\n",
        "  ann_extr_file_pairs[name] = [ann, extr]"
      ],
      "metadata": {
        "id": "SoOaEImtV_UN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_extr_ann_file(path, filename, map):\n",
        "  pd.DataFrame.from_records(map).to_csv(os.path.join(path, filename+\".xlsx\"))\n",
        "\n",
        "  with io.open(os.path.join(path, filename+\".json\"), 'w', encoding='utf-8') as f:\n",
        "    f.write(json.dumps(map, ensure_ascii=False))"
      ],
      "metadata": {
        "id": "mja3lGvyV8fM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_ann_extr_all(path, extractions_path, extr, ann,filename ):\n",
        "  with open(os.path.join(path,extractions_path, extr ), \"r\", encoding='UTF-8') as f:\n",
        "    contents = f.readlines()\n",
        "    extractions = json.loads(contents[0])\n",
        "  with open(os.path.join(path, ann ), \"r\", encoding='UTF-8') as f:\n",
        "    contents = f.read()\n",
        "    annotations = json.loads(contents)\n",
        "\n",
        "  print(len(annotations), len(extractions[\"mentions\"]), len(extractions[\"documents\"]))\n",
        "  doc_sentence_map = {}\n",
        "  linear_order = 1\n",
        "  doc_ids = sorted(list(extractions[\"documents\"].keys()))\n",
        "  for doc_id in doc_ids:\n",
        "    #print(doc_id, document)\n",
        "    document = extractions[\"documents\"][doc_id]\n",
        "    for i,sentence in enumerate(document['sentences']):\n",
        "      doc_sentence_map[(doc_id,i)] = {\"sentence_text\":sentence['words'], \"sno\":linear_order}\n",
        "      linear_order += 1\n",
        "\n",
        "  doc_event_map = []\n",
        "  event_doc_map = {}\n",
        "  for mention in extractions['mentions']:\n",
        "    # if mention['id'].startswith(\"E:\"):\n",
        "    for att in mention[\"attachments\"]:\n",
        "      if \"pageNum\" in att.keys():\n",
        "        this_text = doc_sentence_map[(mention['document'], mention['sentence'])]['sentence_text']\n",
        "        this_linear_order = doc_sentence_map[(mention['document'], mention['sentence'])]['sno']\n",
        "\n",
        "        doc_event_map.append({\"doc_id\":mention['document'], \"pg_num\":att[\"pageNum\"][0],\"blk_id\":att[\"blockIdx\"][0],\"sentence_id\":mention['sentence'], \n",
        "                              \"doc_sentence_count\":len(extractions[\"documents\"][mention['document']][\"sentences\"]), \n",
        "                              \"event_id\":mention[\"id\"], \"event\":mention[\"text\"] }) #corrected_sent_number -> 1,2,5,4,6,7, => 1,2,3,4,5,6\n",
        "        event_doc_map[mention[\"id\"]] = {\"doc_id\":mention['document'],\"pg_num\":att[\"pageNum\"][0],\"blk_id\":att[\"blockIdx\"][0],\"sentence_id\":mention['sentence'], \n",
        "                              \"doc_sentence_count\":len(extractions[\"documents\"][mention['document']][\"sentences\"]), \n",
        "                              \"event_id\":mention[\"id\"], \"event\":mention[\"text\"] , \"sentence_text\":this_text, \"doc_sent_linear_order\":this_linear_order}\n",
        "  event_ann_map = {}\n",
        "  for ann in annotations:\n",
        "    event_ann_map[ann['eventId']] = {\"annotated_page_num\":ann[\"page_num\"],\"para_num\":ann[\"para_num\"], \"event\":ann[\"event\"], 'locationContext': ann['locationContext'],\n",
        "    'temporalContext': ann['temporalContext'],'explanation': ann['explanation']}\n",
        "  \n",
        "  empty_map = {\"annotated_page_num\":\"\",\"para_num\":\"\", \"event\":\"\", 'locationContext': \"\",\n",
        "    'temporalContext': \"\",'explanation': \"\"}\n",
        "  event_extr_ann_map = []\n",
        "  for event, values in event_doc_map.items():\n",
        "    this_event = event_ann_map[event] if event in event_ann_map.keys() else empty_map\n",
        "    event_extr_ann_map.append({\"doc_id\":values['doc_id'],\"annotated_page_num\":this_event[\"annotated_page_num\"],\"para_num\":this_event[\"para_num\"], \"event_id\":values[\"event_id\"],\n",
        "                               \"event\":this_event[\"event\"], 'locationContext': \",\".join(this_event['locationContext']),\n",
        "    \"sentence_text\":\",\".join(values[\"sentence_text\"]),\n",
        "    'temporalContext': \",\".join(this_event['temporalContext']),'explanation': this_event['explanation'], 'pg_num':values['pg_num'], 'blk_id':values['blk_id'], \n",
        "    'sentence_id':values['sentence_id'], 'doc_sentence_count':values['doc_sentence_count'], \"doc_sent_linear_order\":values[\"doc_sent_linear_order\"]})\n",
        "\n",
        "\n",
        "  df = pd.DataFrame.from_records(event_extr_ann_map)\n",
        "  df = df[df.columns]\n",
        "  print(df.columns.to_list())\n",
        "  print(\"\\n\")\n",
        "  df['locationContext'] = df['locationContext'].replace({\"^'|'$\": \"\"}, regex=True)\n",
        "  df['temporalContext'] = df['temporalContext'].replace({\"^'|'$\": \"\"}, regex=True)\n",
        "  df.sort_values(by=['doc_id', 'pg_num','blk_id', 'sentence_id'], inplace=True,\n",
        "               ascending = [True, True, True, True])\n",
        "  d = df[['doc_id', 'pg_num','blk_id','sentence_id', 'doc_sentence_count']]\n",
        "  df['linear_order'] = [i for i in range( 1,len(d)+1, 1)]\n",
        "  print(\"columns \\n\",df.columns)\n",
        "  df.to_csv(os.path.join(path, filename+\"_linear_order_5_17\"+\".xlsx\"))\n",
        "  save_extr_ann_file(path, filename, event_extr_ann_map)\n",
        "  "
      ],
      "metadata": {
        "id": "u7JOxM-imDT3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Uncomment following lines to combine annotations and extractions"
      ],
      "metadata": {
        "id": "Tq4hdGvdWYlo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for key in paper_names:\n",
        "  name = key\n",
        "  ann, extr = ann_extr_file_pairs[name]\n",
        "  combine_ann_extr_all(path, extractions_path, extr, ann, name[:-4]+\"_all\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OoiD5HOWMdW",
        "outputId": "79098f51-24e0-49ad-c75f-ce1baac71a9c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "174 6212 91\n",
            "['doc_id', 'annotated_page_num', 'para_num', 'event_id', 'event', 'locationContext', 'sentence_text', 'temporalContext', 'explanation', 'pg_num', 'blk_id', 'sentence_id', 'doc_sentence_count', 'doc_sent_linear_order']\n",
            "\n",
            "\n",
            "columns \n",
            " Index(['doc_id', 'annotated_page_num', 'para_num', 'event_id', 'event',\n",
            "       'locationContext', 'sentence_text', 'temporalContext', 'explanation',\n",
            "       'pg_num', 'blk_id', 'sentence_id', 'doc_sentence_count',\n",
            "       'doc_sent_linear_order', 'linear_order'],\n",
            "      dtype='object')\n",
            "302 10045 76\n",
            "['doc_id', 'annotated_page_num', 'para_num', 'event_id', 'event', 'locationContext', 'sentence_text', 'temporalContext', 'explanation', 'pg_num', 'blk_id', 'sentence_id', 'doc_sentence_count', 'doc_sent_linear_order']\n",
            "\n",
            "\n",
            "columns \n",
            " Index(['doc_id', 'annotated_page_num', 'para_num', 'event_id', 'event',\n",
            "       'locationContext', 'sentence_text', 'temporalContext', 'explanation',\n",
            "       'pg_num', 'blk_id', 'sentence_id', 'doc_sentence_count',\n",
            "       'doc_sent_linear_order', 'linear_order'],\n",
            "      dtype='object')\n",
            "42 4429 47\n",
            "['doc_id', 'annotated_page_num', 'para_num', 'event_id', 'event', 'locationContext', 'sentence_text', 'temporalContext', 'explanation', 'pg_num', 'blk_id', 'sentence_id', 'doc_sentence_count', 'doc_sent_linear_order']\n",
            "\n",
            "\n",
            "columns \n",
            " Index(['doc_id', 'annotated_page_num', 'para_num', 'event_id', 'event',\n",
            "       'locationContext', 'sentence_text', 'temporalContext', 'explanation',\n",
            "       'pg_num', 'blk_id', 'sentence_id', 'doc_sentence_count',\n",
            "       'doc_sent_linear_order', 'linear_order'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_doc_id_sentence_text(path, extractions_path, extr,filename ):\n",
        "  with open(os.path.join(path,extractions_path, extr ), \"r\", encoding='UTF-8') as f:\n",
        "    contents = f.readlines()\n",
        "    extractions = json.loads(contents[0])\n",
        "\n",
        "  print(len(extractions[\"documents\"]))\n",
        "  doc_sentence_map = []\n",
        "  linear_order = 1\n",
        "  doc_ids = sorted(list(extractions[\"documents\"].keys()))\n",
        "  for doc_id in doc_ids:\n",
        "    #print(doc_id, document)\n",
        "    document = extractions[\"documents\"][doc_id]\n",
        "    for i,sentence in enumerate(document['sentences']):\n",
        "      doc_sentence_map.append({\"doc_id\":doc_id,\"sentence_id\":i,\"sentence_text\":\" \".join(sentence['words']),\n",
        "                               \"raw_text\":\" \".join(sentence['raw']), \"text\":document['text'], \"sno\":linear_order})\n",
        "      if len(sentence['words']) != len(sentence['raw']):\n",
        "        print(\"words and raw words lengths are not equal!\")\n",
        "      linear_order += 1\n",
        "  pd.DataFrame.from_records(doc_sentence_map).to_csv(os.path.join(path, filename+\".csv\"))"
      ],
      "metadata": {
        "id": "pu_nICTlzASA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key in paper_names:\n",
        "  name = key\n",
        "  ann, extr = ann_extr_file_pairs[name]\n",
        "  get_doc_id_sentence_text(path, extractions_path, extr, name[:-4]+\"_all\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5H6w0Xw1YYl",
        "outputId": "2f4b0601-5f4e-4b90-959a-eec89e2bf8ce"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "91\n",
            "76\n",
            "47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filenames = [\"sarsdouble_all.xlsx\", \"modeling_covid_italy_all.xlsx\", \"response-to-covid-19-was-italy-unprepared_all.xlsx\"]"
      ],
      "metadata": {
        "id": "SXQYtok4WNnk"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save all event extractions with linear orders"
      ],
      "metadata": {
        "id": "TY7fhD8CWqX8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filenames = [\"sarsdouble_all_linear_order_5_17.xlsx\", \"modeling_covid_italy_all_linear_order_5_17.xlsx\", \"response-to-covid-19-was-italy-unprepared_all_linear_order_5_17.xlsx\"]\n",
        "diff_distance_map = []\n",
        "for filename in filenames:\n",
        "  print(\"\\n**********************************************************************************\\n\")\n",
        "  print(\"The annotated extractions file name %s \" %(filename))\n",
        "  df = pd.read_csv(os.path.join(path, filename), index_col=False)\n",
        "  df = df[df.columns[1:]]\n",
        "  print(df.columns.to_list())\n",
        "  print(\"\\n\")\n",
        "  event_df = df[df['event_id'].str.startswith(\"E:\")]\n",
        "  print(len(event_df), len(df))\n",
        "  f = filename.replace(\"-\",\"_\").split(\"_\")\n",
        "  f = \"_\".join(f[:f.index(\"all\")])\n",
        "  print(f,f+\"_event.xlsx\")\n",
        "  event_df.sort_values(by=['doc_id', 'pg_num','blk_id', 'sentence_id', 'linear_order'], inplace=True,\n",
        "               ascending = [True, True, True, True, True])\n",
        "  event_df.to_csv(os.path.join(path, f+\"_events.csv\"))\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rldrBSPvfaUI",
        "outputId": "3aab0791-2acd-4ad3-e0aa-f0eda7e9965e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "**********************************************************************************\n",
            "\n",
            "The annotated extractions file name sarsdouble_all_linear_order_5_17.xlsx \n",
            "['doc_id', 'annotated_page_num', 'para_num', 'event_id', 'event', 'locationContext', 'sentence_text', 'temporalContext', 'explanation', 'pg_num', 'blk_id', 'sentence_id', 'doc_sentence_count', 'doc_sent_linear_order', 'linear_order']\n",
            "\n",
            "\n",
            "172 6108\n",
            "sarsdouble sarsdouble_event.xlsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-8d23addc7c87>:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  event_df.sort_values(by=['doc_id', 'pg_num','blk_id', 'sentence_id', 'linear_order'], inplace=True,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "**********************************************************************************\n",
            "\n",
            "The annotated extractions file name modeling_covid_italy_all_linear_order_5_17.xlsx \n",
            "['doc_id', 'annotated_page_num', 'para_num', 'event_id', 'event', 'locationContext', 'sentence_text', 'temporalContext', 'explanation', 'pg_num', 'blk_id', 'sentence_id', 'doc_sentence_count', 'doc_sent_linear_order', 'linear_order']\n",
            "\n",
            "\n",
            "302 10045\n",
            "modeling_covid_italy modeling_covid_italy_event.xlsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-8d23addc7c87>:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  event_df.sort_values(by=['doc_id', 'pg_num','blk_id', 'sentence_id', 'linear_order'], inplace=True,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "**********************************************************************************\n",
            "\n",
            "The annotated extractions file name response-to-covid-19-was-italy-unprepared_all_linear_order_5_17.xlsx \n",
            "['doc_id', 'annotated_page_num', 'para_num', 'event_id', 'event', 'locationContext', 'sentence_text', 'temporalContext', 'explanation', 'pg_num', 'blk_id', 'sentence_id', 'doc_sentence_count', 'doc_sent_linear_order', 'linear_order']\n",
            "\n",
            "\n",
            "42 4429\n",
            "response_to_covid_19_was_italy_unprepared response_to_covid_19_was_italy_unprepared_event.xlsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-8d23addc7c87>:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  event_df.sort_values(by=['doc_id', 'pg_num','blk_id', 'sentence_id', 'linear_order'], inplace=True,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "event_df[['doc_id','pg_num', 'blk_id','sentence_id', 'doc_sentence_count', 'linear_order']]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GATEvNSDYUih",
        "outputId": "d3614878-30de-44bc-8c19-b2230141b333"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          doc_id  pg_num  blk_id  sentence_id  doc_sentence_count  \\\n",
              "1916 -1949274406       2       4            3                  13   \n",
              "2102 -1949274406       2       4           10                  13   \n",
              "2156 -1949274406       2       4           12                  13   \n",
              "1519 -1810507189       8       2            1                  15   \n",
              "1588 -1810507189       8       2            5                  15   \n",
              "1618 -1810507189       8       2            8                  15   \n",
              "1680 -1810507189       8       2           11                  15   \n",
              "1198 -1675118546      11       2            1                   9   \n",
              "1213 -1675118546      11       2            2                   9   \n",
              "1294 -1675118546      11       2            6                   9   \n",
              "1101 -1657329247       5       5            2                   6   \n",
              "1119 -1657329247       5       5            3                   6   \n",
              "890  -1468384437       8       4            1                   6   \n",
              "904  -1468384437       8       4            2                   6   \n",
              "631  -1113525426      13       1           16                  39   \n",
              "358  -1045735558       1       6            0                   2   \n",
              "94    -100047078       7       3            4                  12   \n",
              "100   -100047078       7       3            4                  12   \n",
              "188   -100047078       7       3            8                  12   \n",
              "189   -100047078       7       3            8                  12   \n",
              "241   -100047078       7       3            8                  12   \n",
              "4332   859959020       4       2           17                  23   \n",
              "4360   859959020       4       2           20                  23   \n",
              "4396   859959020       4       2           22                  23   \n",
              "2372  1212143804       5       3            2                   3   \n",
              "2405  1379080646       3       2            5                  21   \n",
              "2446  1379080646       3       2            9                  21   \n",
              "2675  1441143227       1      11            1                   8   \n",
              "2723  1441143227       1      11            4                   8   \n",
              "2724  1441143227       1      11            4                   8   \n",
              "2886  1673799886       9       1            2                   5   \n",
              "2902  1673799886       9       1            4                   5   \n",
              "2903  1673799886       9       1            4                   5   \n",
              "2969  1685655814       2       3            2                  14   \n",
              "3073  1685655814       2       3            5                  14   \n",
              "3077  1685655814       2       3            6                  14   \n",
              "3141  1685655814       2       3            9                  14   \n",
              "3164  1685655814       2       3            9                  14   \n",
              "3196  1685655814       2       3           11                  14   \n",
              "3221  1685655814       2       3           12                  14   \n",
              "3410  1864928046      10       1            8                  21   \n",
              "3594  1864928046      10       1           13                  21   \n",
              "\n",
              "      linear_order  \n",
              "1916          1917  \n",
              "2102          2103  \n",
              "2156          2157  \n",
              "1519          1520  \n",
              "1588          1589  \n",
              "1618          1619  \n",
              "1680          1681  \n",
              "1198          1199  \n",
              "1213          1214  \n",
              "1294          1295  \n",
              "1101          1102  \n",
              "1119          1120  \n",
              "890            891  \n",
              "904            905  \n",
              "631            632  \n",
              "358            359  \n",
              "94              95  \n",
              "100            101  \n",
              "188            189  \n",
              "189            190  \n",
              "241            242  \n",
              "4332          4333  \n",
              "4360          4361  \n",
              "4396          4397  \n",
              "2372          2373  \n",
              "2405          2406  \n",
              "2446          2447  \n",
              "2675          2676  \n",
              "2723          2724  \n",
              "2724          2725  \n",
              "2886          2887  \n",
              "2902          2903  \n",
              "2903          2904  \n",
              "2969          2970  \n",
              "3073          3074  \n",
              "3077          3078  \n",
              "3141          3142  \n",
              "3164          3165  \n",
              "3196          3197  \n",
              "3221          3222  \n",
              "3410          3411  \n",
              "3594          3595  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f5c476bc-c590-42b9-9a07-9dd5d9ef9fc5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>doc_id</th>\n",
              "      <th>pg_num</th>\n",
              "      <th>blk_id</th>\n",
              "      <th>sentence_id</th>\n",
              "      <th>doc_sentence_count</th>\n",
              "      <th>linear_order</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1916</th>\n",
              "      <td>-1949274406</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>13</td>\n",
              "      <td>1917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2102</th>\n",
              "      <td>-1949274406</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>10</td>\n",
              "      <td>13</td>\n",
              "      <td>2103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2156</th>\n",
              "      <td>-1949274406</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>13</td>\n",
              "      <td>2157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1519</th>\n",
              "      <td>-1810507189</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>15</td>\n",
              "      <td>1520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1588</th>\n",
              "      <td>-1810507189</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>15</td>\n",
              "      <td>1589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1618</th>\n",
              "      <td>-1810507189</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>15</td>\n",
              "      <td>1619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1680</th>\n",
              "      <td>-1810507189</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>11</td>\n",
              "      <td>15</td>\n",
              "      <td>1681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1198</th>\n",
              "      <td>-1675118546</td>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>1199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1213</th>\n",
              "      <td>-1675118546</td>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>1214</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1294</th>\n",
              "      <td>-1675118546</td>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>9</td>\n",
              "      <td>1295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1101</th>\n",
              "      <td>-1657329247</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1102</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1119</th>\n",
              "      <td>-1657329247</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>1120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>890</th>\n",
              "      <td>-1468384437</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>891</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>904</th>\n",
              "      <td>-1468384437</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>631</th>\n",
              "      <td>-1113525426</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>39</td>\n",
              "      <td>632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>358</th>\n",
              "      <td>-1045735558</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>359</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>-100047078</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>-100047078</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188</th>\n",
              "      <td>-100047078</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>12</td>\n",
              "      <td>189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>189</th>\n",
              "      <td>-100047078</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>12</td>\n",
              "      <td>190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>241</th>\n",
              "      <td>-100047078</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>12</td>\n",
              "      <td>242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4332</th>\n",
              "      <td>859959020</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>17</td>\n",
              "      <td>23</td>\n",
              "      <td>4333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4360</th>\n",
              "      <td>859959020</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>20</td>\n",
              "      <td>23</td>\n",
              "      <td>4361</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4396</th>\n",
              "      <td>859959020</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>22</td>\n",
              "      <td>23</td>\n",
              "      <td>4397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2372</th>\n",
              "      <td>1212143804</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2373</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2405</th>\n",
              "      <td>1379080646</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>21</td>\n",
              "      <td>2406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2446</th>\n",
              "      <td>1379080646</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>21</td>\n",
              "      <td>2447</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2675</th>\n",
              "      <td>1441143227</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>2676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2723</th>\n",
              "      <td>1441143227</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>2724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2724</th>\n",
              "      <td>1441143227</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>2725</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2886</th>\n",
              "      <td>1673799886</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2902</th>\n",
              "      <td>1673799886</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>2903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2903</th>\n",
              "      <td>1673799886</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>2904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2969</th>\n",
              "      <td>1685655814</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>14</td>\n",
              "      <td>2970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3073</th>\n",
              "      <td>1685655814</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>14</td>\n",
              "      <td>3074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3077</th>\n",
              "      <td>1685655814</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>14</td>\n",
              "      <td>3078</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3141</th>\n",
              "      <td>1685655814</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>9</td>\n",
              "      <td>14</td>\n",
              "      <td>3142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3164</th>\n",
              "      <td>1685655814</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>9</td>\n",
              "      <td>14</td>\n",
              "      <td>3165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3196</th>\n",
              "      <td>1685655814</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>11</td>\n",
              "      <td>14</td>\n",
              "      <td>3197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3221</th>\n",
              "      <td>1685655814</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>12</td>\n",
              "      <td>14</td>\n",
              "      <td>3222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3410</th>\n",
              "      <td>1864928046</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>21</td>\n",
              "      <td>3411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3594</th>\n",
              "      <td>1864928046</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>21</td>\n",
              "      <td>3595</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f5c476bc-c590-42b9-9a07-9dd5d9ef9fc5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f5c476bc-c590-42b9-9a07-9dd5d9ef9fc5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f5c476bc-c590-42b9-9a07-9dd5d9ef9fc5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "filenames = [\"sarsdouble_all_linear_order_5_17.xlsx\", \"modeling_covid_italy_all_linear_order_5_17.xlsx\", \"response-to-covid-19-was-italy-unprepared_all_linear_order_5_17.xlsx\"]\n",
        "loc_context_counter = Counter()\n",
        "temporal_context_counter = Counter()\n",
        "loc_context_counters = {filename: Counter() for filename in filenames}\n",
        "temporal_context_counters = {filename: Counter() for filename in filenames}\n",
        "\n",
        "loc_tf = {}\n",
        "with pd.ExcelWriter(os.path.join(path, \"summarized_context_counter.xlsx\")) as writer:\n",
        "\n",
        "  for filename in filenames:\n",
        "    print(\"\\n**********************************************************************************\\n\")\n",
        "    print(\"The annotated extractions file name %s \" %(filename))\n",
        "    df = pd.read_csv(os.path.join(path, filename), index_col=False)\n",
        "    df = df[df.columns[1:]]\n",
        "    df = df.fillna(\"\")\n",
        "    loc_context_counter = Counter()\n",
        "    temporal_context_counter = Counter()\n",
        "    loc_context_counter = Counter([l for l in sum(df['locationContext'].apply(lambda x: x.split(\",\")).to_list(), []) if l != ''])\n",
        "    temporal_context_counter = Counter([l for l in sum(df['temporalContext'].apply(lambda x: x.split(\",\")).to_list(), []) if l != ''])\n",
        "    print(loc_context_counter)\n",
        "    print(temporal_context_counter)\n",
        "    loc_context_counters[filename] = loc_context_counter\n",
        "    temporal_context_counters[filename] = temporal_context_counter\n",
        "    loc_context_counter.update(temporal_context_counter)\n",
        "    pd.DataFrame.from_dict(loc_context_counter, orient=\"index\").reset_index().to_excel(writer, sheet_name=filename[:-27], index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HODqUVdYl2m",
        "outputId": "18f35f1d-2a64-48e7-b992-3c08de34fc6c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "**********************************************************************************\n",
            "\n",
            "The annotated extractions file name sarsdouble_all_linear_order_5_17.xlsx \n",
            "Counter({'Hong Kong': 128, 'Inner Mongolia': 39, 'Beijing': 39, 'Guangdong': 24, 'Amoy Gardens': 21, 'China': 20, 'Guangdong Province': 7, 'Shanghai': 6, 'Mainland China': 5, 'Europe': 1, 'Paris': 1})\n",
            "Counter({' 2003': 304, 'March 17': 135, 'May 10': 135, '1983': 21, '1985': 21, '03/17': 10, '03/20': 10, '03/23': 10, '03/26': 10, '03/29': 10, '04/01': 10, '04/04': 10, '04/07': 10, '04/10': 10, '04/13': 10, '04/16': 10, '04/19': 10, '04/22': 10, '04/25': 10, '04/28': 10, '05/01': 10, '05/04': 10, '05/07': 10, '05/10': 10, 'February 21st': 9, 'February 21': 9, '17 March': 8, '10 May': 8, 'November 2002': 5, 'February 22': 4})\n",
            "\n",
            "**********************************************************************************\n",
            "\n",
            "The annotated extractions file name modeling_covid_italy_all_linear_order_5_17.xlsx \n",
            "Counter({'Italy': 253, 'Lodi Province': 1, 'Lombardy': 1, 'north to the south of Italy': 1, 'China': 1, 'Italian population': 1, 'Italian society': 1})\n",
            "Counter({'20 February 2020 (day 1) through 5 April 2020 (day 46)': 90, 'after day 50': 44, 'at day 1': 44, 'after day 22': 30, 'day 1': 21, '20 February 2020 (day 1) to 5 April 2020 (day 46)': 14, 'in all cases': 10, 'after day 38': 7, '9 March 2020': 6, '300-day horizon': 5, 'day 22': 4, 'after day 28': 4, 'over a 350-day horizon': 3, 'day 4': 2, 'over a 500-day horizon': 2, '26 February 2020': 1, 'on day 1': 1, 'on day 4': 1, 'at day 12': 1, '22 March 2020': 1, '46': 1, '20 February 2020(day 1) through 5 April 2020 (day 46)': 1, 'after day 12': 1, 'after day 4': 1, '24 February 2020': 1, 'around day 25': 1, 'over a 300-day horizon': 1})\n",
            "\n",
            "**********************************************************************************\n",
            "\n",
            "The annotated extractions file name response-to-covid-19-was-italy-unprepared_all_linear_order_5_17.xlsx \n",
            "Counter({'Italy': 38, 'Lombardy': 8, 'Rome': 4, 'northern Italy': 2, 'European countries and the USA': 2, 'western European countries': 1, 'Spain': 1, 'northern regions': 1, 'Italian': 1, 'Veneto': 1, 'Emilia-Romagna': 1, 'Piedmont': 1, 'Italian regions': 1})\n",
            "Counter({'COVID-19 crisis': 5, 'since the early 1990s': 4, 'between 2000 and 2017': 3, '2020': 3, '2017': 2, '12-week period': 2, 'April 2020': 2, '2019': 2, '2005': 2, 'during the early days of the COVID-19 outbreak': 2, 'between 1st February and 14th April': 2, 'pre-COVID-19': 1, '2014': 1, 'COVID-19 outbreak': 1, 'COVID-19 pandemic': 1, 'COVID-19 health crisis': 1, 'more recently': 1, '2021–2023': 1, '31st January 2020': 1, 'on 3rd February': 1, 'between 31st January 2020': 1, 'beginning of June': 1, 'two months after the beginning of the first wave': 1, '1 January–30 April 2015–2019': 1, 'during the COVID-19 emergency': 1, '2004': 1, '2007': 1, 'early on in the pandemic (March 2020)': 1})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
            "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32Tuzq5Ll3ed",
        "outputId": "1043441a-e2d0-4b88-c226-d9e036bf5a94"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['doc_id', 'annotated_page_num', 'para_num', 'event_id', 'event',\n",
              "       'locationContext', 'sentence_text', 'temporalContext', 'explanation',\n",
              "       'pg_num', 'blk_id', 'sentence_id', 'doc_sentence_count',\n",
              "       'doc_sent_linear_order', 'linear_order'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "filenames = [\"sarsdouble_all_linear_order_5_17.xlsx\", \"modeling_covid_italy_all_linear_order_5_17.xlsx\", \"response-to-covid-19-was-italy-unprepared_all_linear_order_5_17.xlsx\"]\n",
        "\n",
        "loc_tf = {}\n",
        "# with pd.ExcelWriter(os.path.join(path, \"summarized_context_counter.xlsx\")) as writer:\n",
        "\n",
        "for filename in filenames:\n",
        "  print(\"\\n**********************************************************************************\\n\")\n",
        "  print(\"The annotated extractions file name %s \" %(filename))\n",
        "  df = pd.read_csv(os.path.join(path, filename), index_col=False)\n",
        "  df = df[df.columns[1:]]\n",
        "  df = df.fillna(\"\")\n",
        "  \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NT2aJXt6lQsr",
        "outputId": "ccc9e653-98ff-4966-a779-548c1589be16"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "**********************************************************************************\n",
            "\n",
            "The annotated extractions file name sarsdouble_all_linear_order_5_17.xlsx \n",
            "\n",
            "**********************************************************************************\n",
            "\n",
            "The annotated extractions file name modeling_covid_italy_all_linear_order_5_17.xlsx \n",
            "\n",
            "**********************************************************************************\n",
            "\n",
            "The annotated extractions file name response-to-covid-19-was-italy-unprepared_all_linear_order_5_17.xlsx \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame.from_dict(loc_context_counter, orient=\"index\").reset_index()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "x0TglsCzkP18",
        "outputId": "ee42258a-ed65-46e2-ebc9-37a71aebd2dc"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               index   0\n",
              "0                                              Italy  38\n",
              "1                                           Lombardy   8\n",
              "2                                               Rome   4\n",
              "3                         western European countries   1\n",
              "4                                              Spain   1\n",
              "5                                   northern regions   1\n",
              "6                                            Italian   1\n",
              "7                                             Veneto   1\n",
              "8                                     Emilia-Romagna   1\n",
              "9                                           Piedmont   1\n",
              "10                                    northern Italy   2\n",
              "11                                   Italian regions   1\n",
              "12                    European countries and the USA   2\n",
              "13                             between 2000 and 2017   3\n",
              "14                                              2017   2\n",
              "15                                      pre-COVID-19   1\n",
              "16                                              2014   1\n",
              "17                                    12-week period   2\n",
              "18                                              2020   3\n",
              "19                                        April 2020   2\n",
              "20                                 COVID-19 outbreak   1\n",
              "21                                 COVID-19 pandemic   1\n",
              "22                                              2019   2\n",
              "23                            COVID-19 health crisis   1\n",
              "24                                   COVID-19 crisis   5\n",
              "25                                              2005   2\n",
              "26                                     more recently   1\n",
              "27                                         2021–2023   1\n",
              "28                                 31st January 2020   1\n",
              "29                                   on 3rd February   1\n",
              "30                         between 31st January 2020   1\n",
              "31                                 beginning of June   1\n",
              "32  two months after the beginning of the first wave   1\n",
              "33                      1 January–30 April 2015–2019   1\n",
              "34                     during the COVID-19 emergency   1\n",
              "35    during the early days of the COVID-19 outbreak   2\n",
              "36                             since the early 1990s   4\n",
              "37                                              2004   1\n",
              "38                                              2007   1\n",
              "39             early on in the pandemic (March 2020)   1\n",
              "40               between 1st February and 14th April   2"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0b9e082c-8046-4ef0-8bf1-256671cadfb9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Italy</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Lombardy</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Rome</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>western European countries</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Spain</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>northern regions</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Italian</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Veneto</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Emilia-Romagna</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Piedmont</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>northern Italy</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Italian regions</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>European countries and the USA</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>between 2000 and 2017</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2017</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>pre-COVID-19</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>2014</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>12-week period</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>2020</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>April 2020</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>COVID-19 outbreak</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>COVID-19 pandemic</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>2019</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>COVID-19 health crisis</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>COVID-19 crisis</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>2005</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>more recently</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>2021–2023</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>31st January 2020</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>on 3rd February</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>between 31st January 2020</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>beginning of June</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>two months after the beginning of the first wave</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>1 January–30 April 2015–2019</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>during the COVID-19 emergency</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>during the early days of the COVID-19 outbreak</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>since the early 1990s</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>2004</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>2007</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>early on in the pandemic (March 2020)</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>between 1st February and 14th April</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0b9e082c-8046-4ef0-8bf1-256671cadfb9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0b9e082c-8046-4ef0-8bf1-256671cadfb9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0b9e082c-8046-4ef0-8bf1-256671cadfb9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.fillna(\"\")\n",
        "Counter([l for l in sum(df['locationContext'].apply(lambda x: x.split(\",\")).to_list(), []) if l != ''])\n",
        "Counter([l for l in sum(df['temporalContext'].apply(lambda x: x.split(\",\")).to_list(), []) if l != ''])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIzXdqLxcdjQ",
        "outputId": "a3ee1849-b292-426a-ab64-7f05473b1a87"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'between 2000 and 2017': 3,\n",
              "         '2017': 2,\n",
              "         'pre-COVID-19': 1,\n",
              "         '2014': 1,\n",
              "         '12-week period': 2,\n",
              "         '2020': 3,\n",
              "         'April 2020': 2,\n",
              "         'COVID-19 outbreak': 1,\n",
              "         'COVID-19 pandemic': 1,\n",
              "         '2019': 2,\n",
              "         'COVID-19 health crisis': 1,\n",
              "         'COVID-19 crisis': 5,\n",
              "         '2005': 2,\n",
              "         'more recently': 1,\n",
              "         '2021–2023': 1,\n",
              "         '31st January 2020': 1,\n",
              "         'on 3rd February': 1,\n",
              "         'between 31st January 2020': 1,\n",
              "         'beginning of June': 1,\n",
              "         'two months after the beginning of the first wave': 1,\n",
              "         '1 January–30 April 2015–2019': 1,\n",
              "         'during the COVID-19 emergency': 1,\n",
              "         'during the early days of the COVID-19 outbreak': 2,\n",
              "         'since the early 1990s': 4,\n",
              "         '2004': 1,\n",
              "         '2007': 1,\n",
              "         'early on in the pandemic (March 2020)': 1,\n",
              "         'between 1st February and 14th April': 2})"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explore TF-IDF solution and N-GRAM solution since contexts can be phrases"
      ],
      "metadata": {
        "id": "0D1R1ibr3d7C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exploring TF-IDF for finding closest context"
      ],
      "metadata": {
        "id": "nKKl5aiE4CnO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "filenames = [\"sarsdouble_all.csv\", \"modeling_covid_italy_all.csv\", \"response-to-covid-19-was-italy-unprepared_all.csv\"]\n",
        "events_filenames = {\"sarsdouble_all.csv\":\"sarsdouble_events.csv\", \"modeling_covid_italy_all.csv\":\"modeling_covid_italy_events.csv\", \n",
        "                    \"response-to-covid-19-was-italy-unprepared_all.csv\":\"response_to_covid_19_was_italy_unprepared_events.csv\"}\n",
        "\n",
        "loc_tf = {}\n",
        "# with pd.ExcelWriter(os.path.join(path, \"summarized_context_counter.xlsx\")) as writer:\n",
        "\n",
        "for filename in filenames:\n",
        "  print(\"\\n**********************************************************************************\\n\")\n",
        "  print(\"The annotated extractions file name %s \" %(filename))\n",
        "  df = pd.read_csv(os.path.join(path, filename), index_col=False)\n",
        "\n",
        "  sentences = df['sentence_text'].to_list()\n",
        "  count_vectorizer = CountVectorizer()\n",
        "  count_vectorizer.fit_transform(sentences)\n",
        "  print (\"Vocabulary: \")\n",
        "  print(count_vectorizer.vocabulary_)\n",
        "  vocab = list(count_vectorizer.vocabulary_)\n",
        "  print(vocab)\n",
        "\n",
        "\n",
        "  events_df = pd.read_csv(os.path.join(path, events_filenames[filename]), index_col=False)\n",
        "  events_df.fillna(\"\")\n",
        "  events_df['locationContext'] = events_df['locationContext'].replace({\"^'|'$\": \"\"}, regex=True)\n",
        "  events_df['temporalContext'] = events_df['temporalContext'].replace({\"^'|'$\": \"\"}, regex=True)\n",
        "  events_df['locationContext'] = events_df['locationContext'].str.replace('nan', \"\")\n",
        "  events_df['locationContext'] = events_df['locationContext'].str.replace('\\'','')\n",
        "  events_df['temporalContext'] = events_df['temporalContext'].str.replace('nan', \"\")\n",
        "  events_df['temporalContext'] = events_df['temporalContext'].str.replace('\\'','')\n",
        "  \n",
        "  location_contexts = []\n",
        "  temporal_contexts = []\n",
        "  for context in events_df[\"locationContext\"].str.replace('nan', \"\").tolist():\n",
        "    context = str(context).replace(\"nan\", \"\").lower() if type(context) == float else context.lower()\n",
        "    location_contexts.extend(context.split(\",\")) \n",
        "  for context in events_df[\"temporalContext\"].str.replace('nan', \"\").tolist():\n",
        "    context = str(context).replace(\"nan\", \"\").lower() if type(context) == float else context.lower()\n",
        "    temporal_contexts.extend(context.split(\",\"))\n",
        "  location_contexts = list(set(location_contexts))\n",
        "  temporal_contexts = list(set(temporal_contexts))\n",
        "  print(len(location_contexts), len(set(location_contexts)))\n",
        "  print(len(temporal_contexts), len(set(temporal_contexts)))\n",
        "\n",
        "  freq_term_matrix = count_vectorizer.transform(location_contexts)\n",
        "  print(\"Frequency Term matrix\")\n",
        "  print (freq_term_matrix.todense())\n",
        "\n",
        "  count_array = freq_term_matrix.toarray()\n",
        "  df = pd.DataFrame(data=count_array, columns=vocab)\n",
        "  #print(df)\n",
        "\n",
        "  tfidf = TfidfTransformer(norm=\"l2\")\n",
        "  tfidf.fit(freq_term_matrix)\n",
        "  print (\"IDF:\")\n",
        "  print(tfidf.idf_)\n",
        "  freq_term_matrix = count_vectorizer.transform(temporal_contexts)\n",
        "  print(\"Frequency Term matrix\")\n",
        "  print (freq_term_matrix.todense())\n",
        "\n",
        "  count_array = freq_term_matrix.toarray()\n",
        "  df = pd.DataFrame(data=count_array, columns=vocab)\n",
        "  #print(df)\n",
        "\n",
        "  tfidf = TfidfTransformer(norm=\"l2\")\n",
        "  tfidf.fit(freq_term_matrix)\n",
        "  print (\"IDF:\")\n",
        "  print(tfidf.idf_)\n",
        "  #print(df.head)\n",
        "  "
      ],
      "metadata": {
        "id": "I-yRFr_6EiOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### N-Gram solution since contexts can be phrases"
      ],
      "metadata": {
        "id": "WI6FLTUd4OYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmUqinAVbI_F",
        "outputId": "d89ec872-80d3-45f1-c9fb-48ab9ace2ce5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.collocations import *\n",
        "\n",
        "filenames = [\"sarsdouble_all.csv\", \"modeling_covid_italy_all.csv\", \"response-to-covid-19-was-italy-unprepared_all.csv\"]\n",
        "events_filenames = {\"sarsdouble_all.csv\":\"sarsdouble_events.csv\", \"modeling_covid_italy_all.csv\":\"modeling_covid_italy_events.csv\", \n",
        "                    \"response-to-covid-19-was-italy-unprepared_all.csv\":\"response_to_covid_19_was_italy_unprepared_events.csv\"}\n",
        "\n",
        "loc_tf = {}\n",
        "# with pd.ExcelWriter(os.path.join(path, \"summarized_context_counter.xlsx\")) as writer:\n",
        "\n",
        "for filename in filenames:\n",
        "  print(\"\\n**********************************************************************************\\n\")\n",
        "  print(\"The annotated extractions file name %s \" %(filename))\n",
        "  df = pd.read_csv(os.path.join(path, filename), index_col=False)\n",
        "\n",
        "  words = df['sentence_text'].to_list()\n",
        "  sentences = []\n",
        "  for sent in df['text'].to_list():\n",
        "    sentences.extend(sent.split(\"\\n\"))\n",
        "\n",
        "  events_df = pd.read_csv(os.path.join(path, events_filenames[filename]), index_col=False)\n",
        "  events_df.fillna(\"\")\n",
        "  events_df['locationContext'] = events_df['locationContext'].replace({\"^'|'$\": \"\"}, regex=True)\n",
        "  # events_df['temporalContext'] = events_df['temporalContext'].replace({\"^'|'$\": \"\"}, regex=True)\n",
        "  events_df['locationContext'] = events_df['locationContext'].str.replace('nan', \"\")\n",
        "  events_df['locationContext'] = events_df['locationContext'].str.replace('\\'','')\n",
        "  # events_df['temporalContext'] = events_df['temporalContext'].str.replace('nan', \"\")\n",
        "  # events_df['temporalContext'] = events_df['temporalContext'].str.replace('\\'','')\n",
        "  \n",
        "  location_contexts = []\n",
        "  temporal_contexts = []\n",
        "  for context in events_df[\"locationContext\"].str.replace('nan', \"\").tolist():\n",
        "    context = str(context).replace(\"nan\", \"\").lower() if type(context) == float else context.lower()\n",
        "    location_contexts.extend(context.split(\",\")) \n",
        "  for context in events_df[\"temporalContext\"].str.replace('nan', \"\").tolist():\n",
        "    context = str(context).replace(\"nan\", \"\").lower() if type(context) == float else context.lower()\n",
        "    temporal_contexts.extend(context.split(\",\"))\n",
        "  location_contexts = list(set(location_contexts))\n",
        "  temporal_contexts = list(set(temporal_contexts))\n",
        "  print(len(location_contexts), len(set(location_contexts)))\n",
        "  print(len(temporal_contexts), len(set(temporal_contexts)))\n",
        "\n",
        "  tokens = [word.casefold() for sentence in sentences for sent in sent_tokenize(sentence) for word in word_tokenize(sent)]\n",
        "  word_fd = nltk.FreqDist(tokens)\n",
        "  bigram_fd = nltk.FreqDist(nltk.bigrams(tokens))\n",
        "  finder = BigramCollocationFinder(word_fd, bigram_fd)\n",
        "\n",
        "  word_fd = nltk.FreqDist(tokens)\n",
        "  finder = BigramCollocationFinder(word_fd, bigram_fd)\n",
        "  bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
        "  scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
        "  #print(finder.nbest(bigram_measures.pmi, 5))\n",
        "  #print(scored)\n",
        "\n",
        "  fourgram_measures = nltk.collocations.QuadgramAssocMeasures()\n",
        "  finder_4grams = QuadgramCollocationFinder.from_words(tokens)\n",
        "  scored_4grams = finder_4grams.score_ngrams(fourgram_measures.raw_freq)\n",
        "  print(scored_4grams)\n",
        "  # finder_4grams.apply_word_filter(lambda w: len(w) < 3)"
      ],
      "metadata": {
        "id": "2W7jU8UPXTNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for loc in location_contexts:\n",
        "  if loc:\n",
        "    print(loc)\n",
        "    for t in scored_4grams:\n",
        "      words, score = t\n",
        "      text = \" \".join(words)\n",
        "      if loc in text:\n",
        "        print(loc, text )"
      ],
      "metadata": {
        "id": "8y2579911Ppk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temporal_contexts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3Y4r74HSX6l",
        "outputId": "ab68922a-006b-48a3-aa57-0768e21b8f71"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " 'between 2000 and 2017',\n",
              " 'covid-19 crisis',\n",
              " 'during the early days of the covid-19 outbreak',\n",
              " '31st january 2020',\n",
              " '2005',\n",
              " 'on 3rd february',\n",
              " '1 january–30 april 2015–2019',\n",
              " 'since the early 1990s',\n",
              " 'covid-19 outbreak',\n",
              " 'april 2020',\n",
              " '2019',\n",
              " '2020',\n",
              " '12-week period',\n",
              " 'covid-19 pandemic',\n",
              " 'pre-covid-19',\n",
              " 'between 31st january 2020',\n",
              " '2017',\n",
              " 'beginning of june',\n",
              " 'more recently',\n",
              " '2007',\n",
              " '2021–2023',\n",
              " '2004',\n",
              " 'during the covid-19 emergency',\n",
              " 'early on in the pandemic (march 2020)',\n",
              " 'covid-19 health crisis',\n",
              " 'two months after the beginning of the first wave',\n",
              " '2014',\n",
              " 'between 1st february and 14th april']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "location_contexts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93vNjxOFSZ4D",
        "outputId": "a73c0c29-b59c-4ca4-e966-909029f0b668"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " 'italian regions',\n",
              " 'rome',\n",
              " 'italian',\n",
              " 'piedmont',\n",
              " 'spain',\n",
              " 'european countries and the usa',\n",
              " 'emilia-romagna',\n",
              " 'northern regions',\n",
              " 'northern italy',\n",
              " 'western european countries',\n",
              " 'lombardy',\n",
              " 'italy',\n",
              " 'veneto']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tfidf.idf_), len(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4jfnlGhScLc",
        "outputId": "f381054a-d6e9-40d4-ef7a-106b5faba320"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1803, 1803)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for ctx in location_contexts:\n",
        "  if ctx in count_vectorizer.vocabulary_.keys():\n",
        "    print(ctx,count_vectorizer.vocabulary_[ctx])\n",
        "  else:\n",
        "    print(ctx, \" does not exist in this vocab\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGiqCA0KTRYu",
        "outputId": "8ec94ccc-e1ba-4e87-d75a-1104ea68803f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  does not exist in this vocab\n",
            "italian regions  does not exist in this vocab\n",
            "rome 1454\n",
            "italian 939\n",
            "piedmont 1228\n",
            "spain 1538\n",
            "european countries and the usa  does not exist in this vocab\n",
            "emilia-romagna  does not exist in this vocab\n",
            "northern regions  does not exist in this vocab\n",
            "northern italy  does not exist in this vocab\n",
            "western european countries  does not exist in this vocab\n",
            "lombardy 1013\n",
            "italy 941\n",
            "veneto 1746\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if \"italy\" in count_vectorizer.vocabulary_.keys():\n",
        "  print(\"Italy\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrhaZ2d5ToTq",
        "outputId": "df193d1f-d566-4079-a38f-735c83daf105"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Italy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### numpy way of sorting multiple columns"
      ],
      "metadata": {
        "id": "bJou781dLbyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.array([[9, 2, 3],\n",
        "           [4, 5, 6],\n",
        "           [7, 0, 5]])\n",
        "a1 = a[a[:, 0].argsort()]\n",
        "a2 = a1[a1[:, 1].argsort()]\n",
        "a3 = a2[a2[:, 2].argsort()]\n",
        "a1,a2,a3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2SsoxWCDPLg",
        "outputId": "43c96d48-e03f-45cd-9996-342f51cec6f0"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[4, 5, 6],\n",
              "        [7, 0, 5],\n",
              "        [9, 2, 3]]),\n",
              " array([[7, 0, 5],\n",
              "        [9, 2, 3],\n",
              "        [4, 5, 6]]),\n",
              " array([[9, 2, 3],\n",
              "        [7, 0, 5],\n",
              "        [4, 5, 6]]))"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solution: for searching the contexts\n",
        "- For each pdf in pdf_names:\n",
        "  - First construct contexts dictionary with contexts as keys and doc_id, block_id, sentence_number, **all extractions' linear_order_number** from {this_pdf_name}_events.xlsx\n",
        "  - Get doc_sentence_map with doc_id, sentence_text, sentence_id, doc_sent_linear_order from {this_pdf_name}_all.csv\n",
        "  - For each context key in contexts dictionary :\n",
        "    - this_doc_id, sentence_number from contexts dictionary\n",
        "    - from doc_sentence_map, get all sentences up until **doc_sent_map's doc_sent_linear_order**\n",
        "    - get 3 top closest doc_id, sentence_number pairs where the context was found up until this_doc_id,this_sentence_number\n",
        "    - calculate distance between **doc_sent_map's doc_sent_linear_order** between context dictionary value and doc_sentence_map for each of the nearest context matches\n",
        "- Plot the histogram"
      ],
      "metadata": {
        "id": "QvUX6d_m4hN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "filenames = [\"sarsdouble_all.csv\", \"modeling_covid_italy_all.csv\", \"response-to-covid-19-was-italy-unprepared_all.csv\"]\n",
        "events_filenames = {\"sarsdouble_all.csv\":\"sarsdouble_events.csv\", \"modeling_covid_italy_all.csv\":\"modeling_covid_italy_events.csv\", \n",
        "                    \"response-to-covid-19-was-italy-unprepared_all.csv\":\"response_to_covid_19_was_italy_unprepared_events.csv\"}\n",
        "\n",
        "loc_tf = {}\n",
        "# with pd.ExcelWriter(os.path.join(path, \"summarized_context_counter.xlsx\")) as writer:\n",
        "\n",
        "for filename in filenames:\n",
        "  print(\"\\n**********************************************************************************\\n\")\n",
        "  print(\"The annotated extractions file name %s \" %(filename))\n",
        "  df = pd.read_csv(os.path.join(path, filename), index_col=False)\n",
        "\n",
        "  words = df['sentence_text'].to_list()\n",
        "  sentences = []\n",
        "  for sent in df['text'].to_list():\n",
        "    sentences.extend(sent.split(\"\\n\"))\n",
        "\n",
        "  events_df = pd.read_csv(os.path.join(path, events_filenames[filename]), index_col=False)\n",
        "  events_df.fillna(\"\")\n",
        "  events_df['locationContext'] = events_df['locationContext'].replace({\"^'|'$\": \"\"}, regex=True)\n",
        "  # events_df['temporalContext'] = events_df['temporalContext'].replace({\"^'|'$\": \"\"}, regex=True)\n",
        "  events_df['locationContext'] = events_df['locationContext'].str.replace('nan', \"\")\n",
        "  events_df['locationContext'] = events_df['locationContext'].str.replace('\\'','')\n",
        "  # events_df['temporalContext'] = events_df['temporalContext'].str.replace('nan', \"\")\n",
        "  # events_df['temporalContext'] = events_df['temporalContext'].str.replace('\\'','')\n",
        "  \n",
        "  location_contexts = []\n",
        "  temporal_contexts = []\n",
        "  events_df[\"locationContext\"] = events_df[\"locationContext\"].apply(lambda x: str(x).replace(\"nan\", \"\").lower()\n",
        "                                if type(x) == float else x.lower())\n",
        "  events_df[\"temporalContext\"] = events_df[\"temporalContext\"].apply(lambda x: str(x).replace(\"nan\", \"\").lower()\n",
        "                                if type(x) == float else x.lower())\n",
        "  events_df[\"location_contexts\"] = events_df[\"locationContext\"].apply(lambda x: str(x).replace(\"nan\", \"\").lower().split(\",\") \n",
        "                                if type(x) == float else x.lower().split(\",\"))\n",
        "  events_df[\"temporal_contexts\"] = events_df[\"temporalContext\"].apply(lambda x: str(x).replace(\"nan\", \"\").lower().split(\",\") \n",
        "                                if type(x) == float else x.lower().split(\",\"))\n",
        "  for context in events_df[\"location_contexts\"].tolist():\n",
        "    location_contexts.extend(context) \n",
        "  for context in events_df[\"temporal_contexts\"].tolist():\n",
        "    temporal_contexts.extend(context)\n",
        "  ulocation_contexts = list(set(location_contexts))\n",
        "  utemporal_contexts = list(set(temporal_contexts))\n",
        "  print(len(location_contexts), len(set(location_contexts)))\n",
        "  print(len(temporal_contexts), len(set(temporal_contexts)))\n",
        "  all_contexts = ulocation_contexts + utemporal_contexts\n",
        "  contexts = {}\n",
        "  for ctx in ulocation_contexts:\n",
        "    contexts[ctx] = events_df[events_df[\"locationContext\"].str.contains(ctx)][['doc_id', 'event_id',\n",
        "       'event', 'sentence_text', 'pg_num', 'blk_id', 'sentence_id', 'doc_sent_linear_order', 'linear_order']].values.tolist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gO26ennTEHCJ",
        "outputId": "32a73237-1f3e-42f0-b02a-52ad0c8e62e0"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "**********************************************************************************\n",
            "\n",
            "The annotated extractions file name sarsdouble_all.csv \n",
            "295 12\n",
            "853 31\n",
            "\n",
            "**********************************************************************************\n",
            "\n",
            "The annotated extractions file name modeling_covid_italy_all.csv \n",
            "308 8\n",
            "353 28\n",
            "\n",
            "**********************************************************************************\n",
            "\n",
            "The annotated extractions file name response-to-covid-19-was-italy-unprepared_all.csv \n",
            "64 14\n",
            "49 29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "events_df[\"location_contexts\"] = events_df[\"locationContext\"].apply(lambda x: str(x).replace(\"nan\", \"\").lower().split(\",\") \n",
        "                                if type(x) == float else x.lower().split(\",\"))\n",
        "events_df[\"locationContext\"] = events_df[\"locationContext\"].apply(lambda x: str(x).replace(\"nan\", \"\").lower()\n",
        "                                if type(x) == float else x.lower())"
      ],
      "metadata": {
        "id": "dg7BR_97GvTV"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ctx in ulocation_contexts:\n",
        "  if ctx:\n",
        "    contexts[ctx] "
      ],
      "metadata": {
        "id": "4tR0xm7DIgIF"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ulocation_contexts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vp89CtasIVem",
        "outputId": "dd35532a-3144-4c44-a489-60d60914ee52"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " 'italian regions',\n",
              " 'rome',\n",
              " 'italian',\n",
              " 'piedmont',\n",
              " 'spain',\n",
              " 'european countries and the usa',\n",
              " 'emilia-romagna',\n",
              " 'northern regions',\n",
              " 'northern italy',\n",
              " 'western european countries',\n",
              " 'lombardy',\n",
              " 'italy',\n",
              " 'veneto']"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Counter(location_contexts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMArjPD7KHYJ",
        "outputId": "b27adb4d-f99e-41d1-f22c-bce3076f0b3b"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'italy': 38,\n",
              "         'northern regions': 1,\n",
              "         'rome': 4,\n",
              "         'lombardy': 8,\n",
              "         '': 2,\n",
              "         'western european countries': 1,\n",
              "         'spain': 1,\n",
              "         'italian regions': 1,\n",
              "         'european countries and the usa': 2,\n",
              "         'italian': 1,\n",
              "         'veneto': 1,\n",
              "         'emilia-romagna': 1,\n",
              "         'piedmont': 1,\n",
              "         'northern italy': 2})"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "events_df[events_df[\"locationContext\"].str.contains('italy')]"
      ],
      "metadata": {
        "id": "EFgyClZbIp3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "QtyMDG2rR22d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n**********************************************************************************\\n\")\n",
        "print(\"The annotated extractions file name %s \" %(filenames[2]))\n",
        "df = pd.read_csv(os.path.join(path, filenames[2]), index_col=False)\n",
        "\n",
        "words = df['text'].to_list()\n",
        "sentences = []\n",
        "for sent in df['text'].to_list():\n",
        "  sentences.extend(sent.split(\"\\n\"))\n",
        "for ctx, values in contexts.items():\n",
        "  print(\"context \", ctx)\n",
        "  for val in values:\n",
        "    doc_id = val[0]\n",
        "    sentence_num, doc_sent_linear_order, linear_order = val[6:]\n",
        "    print(doc_id, sentence_num, doc_sent_linear_order, linear_order)\n",
        "    sentences = df[df['sno']<=doc_sent_linear_order][['doc_id','sentence_id','sentence_text', 'sno']].values.tolist()\n",
        "    for sentence in sentences:\n",
        "      if ctx in sentence[2]:\n",
        "        print(\"context \"+ctx,sentence[2],  doc_sent_linear_order - sentence[3])\n",
        "      else:\n",
        "        print(\"None\")"
      ],
      "metadata": {
        "id": "40jqi6mTNTOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contexts['rome']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOs0FgmvXLIm",
        "outputId": "7a31d9c5-7d76-48c6-e444-fe994627b79c"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[-1675118546,\n",
              "  'E:-1325102996',\n",
              "  'Current government instructions are based on a three-tier regional risk assessment',\n",
              "  'Current,government,instructions,are,based,on,a,three-tier,regional,risk,assessment,.',\n",
              "  11,\n",
              "  2,\n",
              "  1,\n",
              "  78,\n",
              "  1199],\n",
              " [-1657329247,\n",
              "  'E:166257913',\n",
              "  'which, if completed, will result in a more than doubled overall pre-pandemic capacity',\n",
              "  'A,further,30,%,expansion,(,almost,2400,beds,),to,the,already,expanded,ICU,bed,numbers,has,been,planned,(,April,2020,),which,,,if,completed,,,will,result,in,a,more,than,doubled,overall,pre-pandemic,capacity,.',\n",
              "  5,\n",
              "  5,\n",
              "  2,\n",
              "  73,\n",
              "  1102],\n",
              " [-100047078,\n",
              "  'E:1158695828',\n",
              "  'number increased',\n",
              "  'Despite,recent,attempts,by,the,Italian,government,to,address,this,imbalance,through,increasing,the,number,of,students,training,to,become,nurse,(,the,number,increased,to,13,000,in,2014,from,a,low,3100,),,,the,COVID-19,crisis,has,heightened,the,shortage,of,health,care,professionals,suffered,by,the,SSN,,,with,a,pre-COVID-19,incidence,of,medical,personnel,of,about,95,workers,per,10,000,inhabitants,(,57,nurses,,,19,doctors,and,19,other,technical,staff,),.',\n",
              "  7,\n",
              "  3,\n",
              "  8,\n",
              "  9,\n",
              "  242],\n",
              " [1441143227,\n",
              "  'E:-1716002397',\n",
              "  'detection of the first two COVID-19 cases',\n",
              "  \"Between,31st,January,2020,,,when,a,state,of,national,emergency,was,declared,after,the,detection,of,the,first,two,COVID-19,cases,in,Rome,,,and,the,easing,of,the,stringent,lockdown,restrictions,at,the,beginning,of,June,,,Italy,was,',hit,by,nothing,short,of,a,tsunami,of,unprecedented,forces,,,punctuated,by,an,incessant,stream,of,deaths,.\",\n",
              "  1,\n",
              "  11,\n",
              "  1,\n",
              "  171,\n",
              "  2676]]"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "events_df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTC_bdqhKaN3",
        "outputId": "53640d2e-16cf-460a-e4b5-b764a59312cd"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Unnamed: 0', 'doc_id', 'annotated_page_num', 'para_num', 'event_id',\n",
              "       'event', 'locationContext', 'sentence_text', 'temporalContext',\n",
              "       'explanation', 'pg_num', 'blk_id', 'sentence_id', 'doc_sentence_count',\n",
              "       'doc_sent_linear_order', 'linear_order', 'location_contexts',\n",
              "       'temporal_contexts'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ignore"
      ],
      "metadata": {
        "id": "Co1f-6KTFTOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/Colab Notebooks/skema/data/\"\n",
        "extractions_path = \"cosmos-and-extractions-jsons-for-3-papers\"\n",
        "annotated_files = [\"data-sars-double.json\", \"data_modeling_covid_italy.json\", \"data-response-to-covid-19-was-italy-unprepared.json\"]\n",
        "extraction_files =[\"extractions_sarsdouble.json\", \"extractions_modeling_covid_italy--COSMOS-data.json\",\"extractions_response-to-covid-19-was-italy-unprepared--COSMOS-data.json\"]\n",
        "paper_names = [\"sarsdouble.pdf\", \"modeling_covid_italy.pdf\", \"response-to-covid-19-was-italy-unprepared.pdf\"]\n",
        "ann_extr_file_pairs = {}\n",
        "for name,ann, extr in zip(paper_names,annotated_files, extraction_files):\n",
        "  ann_extr_file_pairs[name] = [ann, extr]"
      ],
      "metadata": {
        "id": "djXC5AIrC5u9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ann_extr_file_pairs"
      ],
      "metadata": {
        "id": "wlCYrh7AcTRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ann, extr in list(ann_extr_file_pairs.values())[:1]:\n",
        "  print(\"This extractions file \",extr)\n",
        "  with open(os.path.join(path,extractions_path, extr ), \"r\", encoding='UTF-8') as f:\n",
        "    contents = f.readlines()\n",
        "    extractions = json.loads(contents[0])\n",
        "    "
      ],
      "metadata": {
        "id": "KvVcBXp6cSKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# events\n",
        "\n",
        "# eventId\n",
        "# page number , block number, sentence id -page -> linear order\n",
        "\n",
        "# eventid 1,2,3,4,5\n",
        "\n",
        "# linear sentence_order : 6,10,12,12,14\n",
        "\n",
        "# location context\n",
        "# temporal context \n",
        "\n",
        "# 1) missing linear order in TextReadingPipeline output -> extractions json\n",
        "# 2) location context - annotations \n",
        "# \tfor each eventID in eventMentions in annotations\n",
        "\n",
        "# \t\tlocations = [ \"Italy\", \"Rome\"]\n",
        "# \t\tlinear order entire extractions -> linear order for this evntID\n",
        "# \t\t\tsentenceID -> documents and look in sentence text from this sentenceID \n",
        "# \t\t\tand get closest sentenceID where you find this location's text"
      ],
      "metadata": {
        "id": "Z7P614z47MzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ipDy9RniFc_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y1waVgSXFBq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uX7-M1VD3fAP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}